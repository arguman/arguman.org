# -*- coding:utf-8 -*-
import operator
import string
import operator
import itertools

import snowballstemmer
from textblob import TextBlob, Word

LOWER_MAP = {
    'tr': {
        ord('I'): u'Ä±'
    }
}

STEMMERS = {
    'en': snowballstemmer.stemmer('english'),
    'tr': snowballstemmer.stemmer('turkish'),
}


def noun_phrases(text):
    blob = TextBlob(text)
    return blob.tokenize()


def get_synsets(text):
    return Word(to_lemma(text)).synsets


def get_lemmas(text):
    word = Word(to_lemma(text))
    sets = map(set, [synset.lemma_names()
                     for synset in word.synsets])

    return map(from_lemma, reduce(operator.or_, sets))


def to_lemma(text):
    return text.replace(' ', '_')


def from_lemma(text):
    return text.replace('_', ' ')


def stem_word(word, language):
    stemmer = STEMMERS.get(language)

    if stemmer is None:
        return word

    return (stemmer
            .stemWord(word)
            .strip(string.punctuation))


def tokenize(wordlist, language, stem=True):
    return ' '.join((stem_word(word, language) if stem else word)
                    for word in wordlist)


def lower(text, language):
    if language in LOWER_MAP:
        text = text.translate(LOWER_MAP[language])
    return text.lower()


def build_ngrams(text, language='en'):
    blob = TextBlob(lower(text, language))
    ngrams = [blob.ngrams(n=n) for n in (3, 2, 1)]
    wordlists = reduce(operator.add, ngrams)
    tokenized = (
        tokenize(wordlist, language, stem=True)
        for wordlist in wordlists)
    pure = (
        tokenize(wordlist, language, stem=False)
        for wordlist in wordlists)
    return itertools.chain(tokenized, pure)


def is_subsequence(sequence, parent):
    for i in xrange(1 + len(parent) - len(sequence)):
        if sequence == parent[i:i + len(sequence)]:
            return True
    return False
